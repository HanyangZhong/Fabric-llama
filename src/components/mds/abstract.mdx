# Abstract

Choosing the correct fabric is key to meeting functional and quality demands in robotic applications for textile manufacturing, apparel production, and smart retail. This paper introduces MLLM-FaSS, a multimodal large language model-based robotic framework for fabric sorting and selection. The system employs a robotic arm, a camera, a visual-tactile sensor, and a pressure sensor, combining supervised fine-tuning and knowledge distillation to sort and rank fabric properties accurately. We release a dataset of 200 fabric samples, which includes RGB images and synchronized visual-tactile and pressure data, to encourage further research. Experimental results demonstrate that our models surpass baseline models, achieving state-of-the-art fabric sorting and selection performance. 